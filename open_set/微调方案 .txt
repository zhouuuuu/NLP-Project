冻结 (Freeze)：原来的模型 (authorship_model.pt)。

将其最庞大、最耗时的“编码器”主干（encoder）完全冻结，使其在训练中保持不变。这节省了99%的计算资源。

解锁 (Unlock)：模型顶端那两个小巧的“头部”（proj 投影层和 classifier 分类层）。

微调 (Fine-tune)：我们只对这个小“头部”进行了几轮（5个 Epochs）的快速训练。

关键：使用“双重损失” (Dual Loss) 进行微调

在微调时，我们不再只用旧的“分类损失”。我们同时使用了两个损失函数：

1. 分类损失 (Cross-Entropy)： 确保模型仍然能准确区分 9 个作者（例如，正确分出 JaneAusten 和 OscarWilde）。

2. 原型损失 (Prototype Loss)： （这是成功的核心） 这个新损失会“惩罚”那些离同类样本中心太远的嵌入。它就像一个“磁铁”，迫使所有 JaneAusten 的样本在特征空间中紧紧地“吸”在一起，形成一个致密的簇。

最终效果
这个“磁铁”（原型损失）的动作，自然地把所有不属于这 9 个簇的“未知”样本（unknown）“挤”到了簇与簇之间的“空白地带”。

在新 UMAP/t-SNE 图中，unknown 点不再像之前全部混在簇的内部，而是整体分布被“推”到了外部，从而让“原型距离”（Prototype）策略的检测效果大幅提升。